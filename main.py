from fastapi import FastAPI, Query
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup

# ====== –î–û–ë–ê–í–õ–ï–ù–û ======
import concurrent.futures
from urllib.parse import quote
import re, os
from typing import List, Dict

# –û–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã –º–µ–Ω—å—à–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/127.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru,en;q=0.9,zh;q=0.8",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}
SESSION = requests.Session()
SESSION.headers.update(DEFAULT_HEADERS)
TIMEOUT = 12
MAX_WORKERS = 16
MAX_RESULTS = 500

# –ö–õ–Æ–ß–ò (—Å—Ç–∞–≤—å –Ω–∞ Render ‚Üí Environment)
APIFY_TOKEN = os.getenv("APIFY_TOKEN", "").strip()
SERPAPI_KEY = os.getenv("SERPAPI_KEY", "").strip()
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======


app = FastAPI(
    title="DI-Agent SDK",
    description="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –ö–∏—Ç–∞—è",
    version="2.0.0"
)

# üîπ –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
class SearchRequest(BaseModel):
    query: str

# üîπ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
def safe_request(url):
    try:
        r = SESSION.get(url, timeout=TIMEOUT)
        if r.status_code == 200:
            return r.text
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {r.status_code} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
            return ""
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
        return ""

# üîπ –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π (—Ç–≤–æ–π –∏—Å—Ö–æ–¥–Ω—ã–π –ø–∞—Ä—Å–µ—Ä ‚Äî –ù–ï –¢–†–û–ì–ê–õ)
def parse_suppliers(html, selectors, source):
    if not html:
        return []

    soup = BeautifulSoup(html, "lxml")
    titles, links = [], []

    for sel in selectors:
        for t in soup.select(sel["title"]):
            title = t.get_text(strip=True)
            if title and title not in titles:
                titles.append(title)
        for a in soup.select(sel["link"]):
            href = a.get("href")
            if href and href not in links:
                links.append(href)

    suppliers = []
    for i in range(min(5, len(titles))):
        suppliers.append({
            "–ù–∞–∑–≤–∞–Ω–∏–µ": titles[i],
            "–°—Å—ã–ª–∫–∞": links[i] if i < len(links) else "N/A",
            "–ò—Å—Ç–æ—á–Ω–∏–∫": source
        })
    return suppliers

# ====== –î–û–ë–ê–í–õ–ï–ù–û: –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —Å–∞–π—Ç—ã ======
GENERIC_SELECTORS = [
    {"title": "h2 a", "link": "h2 a"},
    {"title": ".title a", "link": ".title a"},
    {"title": ".product-title a", "link": ".product-title a"},
    {"title": ".company-name a", "link": ".company-name a"},
    {"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"},
    {"title": ".organic-gallery-title a", "link": ".organic-gallery-title a"},
    {"title": "h3 a", "link": "h3 a"},
    {"title": "a", "link": "a"},
]

def parse_flexible(html: str, source: str) -> List[Dict]:
    if not html:
        return []
    soup = BeautifulSoup(html, "lxml")
    results: List[Dict] = []
    seen = set()

    for sel in GENERIC_SELECTORS:
        for a in soup.select(sel["link"]):
            href = a.get("href", "").strip()
            title = a.get_text(" ", strip=True)
            if not href or not title:
                continue
            if href.startswith("//"):
                href = "https:" + href
            if not href.startswith("http"):
                continue
            key = (title, href)
            if key in seen:
                continue
            seen.add(key)
            results.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": href,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": source
            })
            if len(results) >= 50:
                break
        if len(results) >= 50:
            break
    return results
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======


@app.get("/")
def root():
    return {
        "status": "‚úÖ DI-Agent SDK –∞–∫—Ç–∏–≤–µ–Ω",
        "docs": "/docs",
        "search_example": "/search?q=–õ–°–¢–ö"
    }


# ====== –î–û–ë–ê–í–õ–ï–ù–û: —Å–ø–∏—Å–æ–∫ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (B2B/–∫–∞—Ç–∞–ª–æ–≥–∏/–ø–æ–∏—Å–∫–∏) ======
SOURCES: Dict[str, str] = {
    # –ö—Ä—É–ø–Ω—ã–µ B2B
    "Alibaba": "https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&searchText={q}",
    "Made-in-China": "https://www.made-in-china.com/search?word={q}",
    "GlobalSources": "https://www.globalsources.com/searchList?query={q}",
    "1688 (via Baidu)": "https://www.baidu.com/s?wd={q}+site:1688.com",
    "HKTDC": "https://sourcing.hktdc.com/Search-Product?keyword={q}",
    "ECVV": "https://www.ecvv.com/catalog/{q}.html",
    "ECER": "https://www.ecer.com/search?kw={q}",
    "HC360": "https://s.hc360.com/seller/search.html?kwd={q}",
    "DHgate": "https://www.dhgate.com/wholesale/search.do?act=search&searchkey={q}",
    "YiwuGo": "https://en.yiwugo.com/search/{q}.html",
    "TradeKey": "https://www.tradekey.com/suppliers/{q}.html",
    "ExportHub": "https://www.exporthub.com/search?q={q}",
    "TradeWheel": "https://www.tradewheel.com/search/{q}/",
    "En.China.cn": "https://en.china.cn/search.html?searchKey={q}",
    "Hisupplier": "https://www.hisupplier.com/wholesale/{q}/",
    "Epoly": "https://www.etwinternational.com/search?kw={q}",
    "Globalspec": "https://www.globalspec.com/Search/Results?query={q}",
    "ThomasNet": "https://www.thomasnet.com/search.html?what={q}",
    "Kompass": "https://us.kompass.com/en/searchCompanies/companies/{q}/",
    "Qcc (companies)": "https://www.qcc.com/web/search?key={q}",
    "Tianyancha": "https://www.tianyancha.com/search?key={q}",

    # –ú–∞—Ä–∫–µ—Ç—ã –ö–∏—Ç–∞—è
    "JD": "https://search.jd.com/Search?keyword={q}",
    "Taobao": "https://s.taobao.com/search?q={q}",
    "Pinduoduo": "https://mobile.yangkeduo.com/search_result.html?search_key={q}",

    # –ü—Ä–æ—Ñ. –∫–∞—Ç–∞–ª–æ–≥–∏/–±–∏—Ä–∂–∏/—Ç–µ–Ω–¥–µ—Ä—ã
    "MFG": "https://www.mfg.com/en/search/?q={q}",
    "AliExpress B2B": "https://www.aliexpress.com/wholesale?SearchText={q}",
    "Globalsources Verified": "https://www.globalsources.com/searchList?query={q}&verifiedSupplier=true",
    "Baidu Baike": "https://baike.baidu.com/search?word={q}",
    "Sohu": "https://www.sogou.com/web?query={q}",
    "Bing China": "https://cn.bing.com/search?q={q}",
    "Google (backup)": "https://www.google.com/search?q={q}",

    # –ï—â—ë B2B/–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
    "E-WorldTrade": "https://www.eworldtrade.com/search/{q}/",
    "China.cn": "https://www.china.cn/search.html?searchKey={q}",
    "B2BManufactures": "https://www.manufacturers.com.tw/search.php?words={q}",
    "Maker-In-China": "https://www.maker-in-china.com/search.html?kw={q}",
    "Manufacturers Directory": "https://www.manufacturersdirectory.com/search?query={q}",
    "IndiaMART": "https://dir.indiamart.com/search.mp?ss={q}",
    "TradeIndia": "https://www.tradeindia.com/search.html?search_text={q}",
    "ECPlaza": "https://www.ecplaza.net/search/1?keyword={q}",
    "YellowPages": "https://www.yellowpages.com/search?search_terms={q}",
    "B2Brazil": "https://b2brazil.com/hotsite/search?term={q}",
    "B2BMit": "https://www.b2bmit.com/search.html?q={q}",
    "Globalsources Suppliers": "https://www.globalsources.com/suppliers?query={q}",
    "AliBaba Suppliers": "https://www.alibaba.com/company_directory/search/{q}.html",
    "CantonFair": "https://www.cantonfair.org.cn/en-US/search?key={q}",
    "HKTDC Suppliers": "https://sourcing.hktdc.com/en/supplier-search/{q}",
    "Europages": "https://www.europages.com/companies/{q}.html",
    "Kompass CN": "https://cn.kompass.com/en/searchCompanies/companies/{q}/",
    "Made-in-China Companies": "https://www.made-in-china.com/company-search/?word={q}",
    "MIC Verified": "https://www.made-in-china.com/company-search/?word={q}&select=verified",
    "GlobalMarket": "https://www.globalmarket.com/search/{q}.html",
    "EtradeAsia": "https://www.etradeasia.com/search?keyword={q}",
    "Mawoo": "https://www.made-in-asia.net/search?kw={q}",
    "EveryChina": "https://www.everychina.com/search.html?kw={q}",
    "ChinaProducts": "https://www.china-products-manufacturers.com/search?keyword={q}",
    "Crov": "https://www.crov.com/search?q={q}",
    "DiyTrade": "https://www.diytrade.com/china/search/products.do?keyword={q}",
    "Okchem": "https://www.okchem.com/search?keyword={q}",
    "ChemNet": "https://www.chemnet.com/global/en/search.html?keyword={q}",
    "Food2China": "https://www.food2china.com/search?keyword={q}",
    "PharmaSources": "https://www.pharmasources.com/searchResult?keyword={q}",
    "MedicaTradeFair": "https://www.medica-tradefair.com/vis/v1/en/search?term={q}",
    "HKTDC Products": "https://sourcing.hktdc.com/Search-Product?keyword={q}&productonly=1",
}

# –¢–æ—á–µ—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫ (–æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä)
SITE_SELECTORS: Dict[str, List[Dict[str, str]]] = {
    "Alibaba": [{"title": ".organic-gallery-title", "link": ".organic-gallery-title a"}],
    "Made-in-China": [{"title": ".company-name a", "link": ".company-name a"}],
    "GlobalSources": [{"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"}],
    "1688 (via Baidu)": [{"title": "h3.t a", "link": "h3.t a"}],
    "HKTDC": [{"title": ".product-name a, .cmpny-name a", "link": ".product-name a, .cmpny-name a"}],
    "ECER": [{"title": ".pro-title a, .supplier-name a", "link": ".pro-title a, .supplier-name a"}],
    "ECVV": [{"title": ".pro-title a, .company a", "link": ".pro-title a, .company a"}],
    "HC360": [{"title": ".search-list .title a", "link": ".search-list .title a"}],
    "DHgate": [{"title": ".item-title a", "link": ".item-title a"}],
    "YiwuGo": [{"title": ".title a", "link": ".title a"}],
    "TradeWheel": [{"title": ".item-title a", "link": ".item-title a"}],
    "ExportHub": [{"title": "h4.media-heading a", "link": "h4.media-heading a"}],
}

# –ë—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–∞—Å—Ç—ã—Ö —Ä—É—Å—Å–∫–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ –∞–Ω–≥–ª-–∑–∞–ø—Ä–æ—Å—ã
def normalize_query(q: str) -> str:
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", q):
        low = q.lower()
        mapping = {
            "—Ñ–∏–±—Ä–æ—Ü–µ–º": "fiber cement panels",
            "–ø–µ–Ω–æ–±–µ—Ç–æ–Ω": "foam concrete",
            "–ª—Å—Ç–∫": "light gauge steel frame",
            "—Å—ç–Ω–¥–≤–∏—á": "sandwich panels",
            "–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω": "galvanized steel",
            "–∞–ª—é–∫–æ–±–æ–Ω–¥": "aluminum composite panel",
        }
        for k, v in mapping.items():
            if k in low:
                return v
    return q

# ====== –î–û–ë–ê–í–õ–ï–ù–û: –°–æ—Ü-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ/—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ API ======
def serpapi_site_search(query: str, site: str, source_name: str) -> List[Dict]:
    """Site: search —á–µ—Ä–µ–∑ SerpAPI (—Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ + —Å–Ω–∏–ø–ø–µ—Ç—ã + –∏–Ω–æ–≥–¥–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏)."""
    if not SERPAPI_KEY:
        return []
    try:
        params = {
            "engine": "google",
            "q": f"site:{site} {query}",
            "num": 10,
            "api_key": SERPAPI_KEY
        }
        r = requests.get("https://serpapi.com/search.json", params=params, timeout=12)
        js = r.json()
        out = []
        for item in js.get("organic_results", []):
            title = item.get("title")
            link = item.get("link")
            if not title or not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title,
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": source_name
            })
        return out
    except Exception as e:
        print(f"‚ùå SerpAPI {source_name}: {e}")
        return []

def apify_instagram_search(query: str) -> List[Dict]:
    """Public Instagram via Apify actor (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É–±–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π/–ø–æ—Å—Ç–æ–≤)."""
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~instagram-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "posts",
            "profilesType": "hashtag",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("caption") or it.get("ownerUsername") or "Instagram result"
            link = it.get("url") or it.get("shortCodeUrl")
            if not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": "Instagram (Apify)"
            })
        return out
    except Exception as e:
        print(f"‚ùå Apify Instagram: {e}")
        return []

def apify_tiktok_search(query: str) -> List[Dict]:
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~tiktok-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "videos",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("desc") or it.get("authorName") or "TikTok result"
            link = it.get("webVideoUrl") or it.get("shareUrl")
            if not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": "TikTok (Apify)"
            })
        return out
    except Exception as e:
        print(f"‚ùå Apify TikTok: {e}")
        return []

def social_collect(query: str) -> List[Dict]:
    q = normalize_query(query)
    results = []

    # Instagram/TikTok (—á–µ—Ä–µ–∑ Apify)
    results += apify_instagram_search(q)
    results += apify_tiktok_search(q)

    # --- –ß–µ—Ä–µ–∑ SerpAPI site: ---
    social_sites = {
        # üîπ –ó–∞–ø–∞–¥–Ω—ã–µ —Å–æ—Ü—Å–µ—Ç–∏
        "Telegram": "t.me",
        "WhatsApp": "wa.me",
        "YouTube": "youtube.com",
        "Facebook": "facebook.com",
        "Twitter (X)": "x.com",
        "Pinterest": "pinterest.com",
        "Reddit": "reddit.com",
        "LinkedIn": "linkedin.com/company",
        "Threads (Meta)": "threads.net",
        "Instagram (backup)": "instagram.com",
        "Snapchat": "snapchat.com",
        "Twitch": "twitch.tv",
        "Discord": "discord.com",
        "Tumblr": "tumblr.com",
        "Medium": "medium.com",

        # üîπ –í–æ—Å—Ç–æ—á–Ω—ã–µ –∏ –∫–∏—Ç–∞–π—Å–∫–∏–µ
        "WeChat": "weixin.qq.com",
        "QQ": "qq.com",
        "Weibo": "weibo.com",
        "Douyin (CN TikTok)": "douyin.com",
        "Bilibili": "bilibili.com",
        "Zhihu": "zhihu.com",
        "Youku": "youku.com",
        "Xiaohongshu (RED)": "xiaohongshu.com",
        "Taobao Live": "live.taobao.com",
        "Kuaishou": "kuaishou.com",

        # üîπ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ
        "VK": "vk.com",
        "Odnok–ª–∞—Å—Å–Ω–∏–∫–∏": "ok.ru",
        "Rutube": "rutube.ru",
        "Yappy": "yappy.media",
        "Dzen": "dzen.ru",
    }

    for name, site in social_sites.items():
        results += serpapi_site_search(q, site, name)

    return results
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======

def _fetch_one(name: str, url: str) -> List[Dict]:
    html = safe_request(url)
    if name in SITE_SELECTORS:
        return parse_suppliers(html, SITE_SELECTORS[name], name)
    return parse_flexible(html, name)

def extended_collect(query: str) -> List[Dict]:
    q_norm = normalize_query(query)
    q_enc = quote(q_norm)

    tasks = {}
    results: List[Dict] = []

    # 1) –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (API) ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    try:
        social = social_collect(q_norm)
        if social:
            print(f"‚úÖ SOCIAL: {len(social)}")
            results.extend(social)
    except Exception as e:
        print(f"‚ùå SOCIAL error: {e}")

    # 2) B2B/–∫–∞—Ç–∞–ª–æ–≥–∏/–ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for name, tmpl in SOURCES.items():
            url = tmpl.format(q=q_enc)
            tasks[ex.submit(_fetch_one, name, url)] = name

        for fut in concurrent.futures.as_completed(tasks):
            name = tasks[fut]
            try:
                chunk = fut.result() or []
                if chunk:
                    print(f"‚úÖ {name}: {len(chunk)}")
                results.extend(chunk)
            except Exception as e:
                print(f"‚ùå {name}: {e}")

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–æ–ª–Ω–æ–π —Å—Å—ã–ª–∫–µ
    uniq, seen = [], set()
    for item in results:
        link = (item.get("–°—Å—ã–ª–∫–∞") or "").strip()
        if not link or link in seen:
            continue
        seen.add(link)
        uniq.append(item)
        if len(uniq) >= MAX_RESULTS:
            break
    return uniq


# üîπ –ü–†–Ø–ú–û–ô —ç–Ω–¥–ø–æ–∏–Ω—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ (HTML –∫–∞—Ä—Ç–æ—á–∫–∏)
from fastapi.responses import HTMLResponse

@app.get("/search_all", response_class=HTMLResponse)
def search_all(q: str = Query(..., description="–ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (HTML –∫–∞—Ä—Ç–æ—á–∫–∏)")):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç ‚Äî –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–æ—Å q (–Ω–∞–ø—Ä–∏–º–µ—Ä, LGSF) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏."""
    data = extended_collect(q)
    html_output = format_for_silent_agent_cards(data, q)
    return html_output


# ====== KEEP-ALIVE (—á—Ç–æ–±—ã Render –Ω–µ –∑–∞—Å—ã–ø–∞–ª) ======
import threading, time

def keep_alive():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–∏–Ω–≥—É–µ—Ç —Å–∞–º —Å–µ—Ä–≤–µ—Ä, —á—Ç–æ–±—ã Render –Ω–µ –∑–∞—Å—ã–ø–∞–ª."""
    while True:
        try:
            requests.get("https://di-agent-sdk.onrender.com/", timeout=5)
            print("üîÑ Keep-alive ping OK")
        except Exception as e:
            print(f"‚ö†Ô∏è Keep-alive error: {e}")
        time.sleep(300)  # –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞ FastAPI
threading.Thread(target=keep_alive, daemon=True).start()
# ====== /KEEP-ALIVE ======


# ====== –î–û–ë–ê–í–õ–ï–ù–û: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏ HTML-–∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è –≤—ã–≤–æ–¥–∞ ======
def extract_contacts(html: str, url: str = "") -> Dict[str, str]:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å –∏–∑ HTML: —Ç–µ–ª–µ—Ñ–æ–Ω—ã, email, —Å–æ—Ü—Å—Å—ã–ª–∫–∏ (WhatsApp/Telegram/WeChat),
    —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã/ICP/—Ä–µ–≥–∏–æ–Ω/MOQ/Price –∏ —Ç.–ø.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ (—Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏).
    """
    out = {
        "Phone": "",
        "Email": "",
        "WhatsApp": "",
        "Telegram": "",
        "WeChat": "",
        "Region": "",
        "Price": "",
        "MOQ": "",
        "Certificates": "",
        "CNAME": "",
        "RawURL": url
    }
    if not html:
        return out

    phone_re = re.compile(r"(\+?\d[\d\-\s\(\)]{6,}\d)")
    email_re = re.compile(r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})")
    whatsapp_re = re.compile(r"(?:wa\.me/|whatsapp(?:\.com)?/send\?phone=)(\+?\d[\d\-]{5,})", re.I)
    telegram_re = re.compile(r"(?:t\.me/|telegram\.me/)([A-Za-z0-9_]{3,})", re.I)
    wechat_re = re.compile(r"(?:weixin\.qq\.com|wxid|wechat|ÂæÆ‰ø°|WeChat)[^\s'\"<>]{0,40}", re.I)

    cert_re = re.compile(r"(ICP[^\s,;:<\)]{1,30}|Â§áÊ°à|certificate|–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç[^\n\r]{0,80})", re.I)
    region_re = re.compile(r"(Region|–†–µ–≥–∏–æ–Ω|City|Province|ÊâÄÂú®Âú∞|ÊâÄÂú®ÁúÅ|–≥–æ—Ä–æ–¥|–≥–æ—Ä–æ–¥:)[\s:-‚Äì]*([A-Za-z–ê-–Ø–∞-—è0-9\-\s,]+)", re.I)
    moq_re = re.compile(r"(MOQ|–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∑–∞–∫–∞–∑|–º–∏–Ω\. –∑–∞–∫–∞–∑|–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º)[^\d]{0,10}([0-9,.\s]+)", re.I)
    price_re = re.compile(r"(?:price|—Ü–µ–Ω–∞|–¶–µ–Ω–∞)[^\d]{0,10}([\d\$\‚Ç¨\¬£\.,\s/]+)", re.I)
    cname_re = re.compile(r"(CNAME|cname)[^\w]{0,3}([A-Za-z0-9\.\-]+)", re.I)

    m_email = email_re.search(html)
    if m_email:
        out["Email"] = m_email.group(1).strip()

    phones = phone_re.findall(html)
    phones = [p.strip() for p in phones if len(re.sub(r"\D", "", p)) >= 6]
    if phones:
        out["Phone"] = phones[0]

    m_wa = whatsapp_re.search(html)
    if m_wa:
        out["WhatsApp"] = m_wa.group(1).strip()
    m_tg = telegram_re.search(html)
    if m_tg:
        out["Telegram"] = m_tg.group(1).strip()
    m_wx = wechat_re.search(html)
    if m_wx:
        out["WeChat"] = m_wx.group(0).strip()

    m_cert = cert_re.search(html)
    if m_cert:
        out["Certificates"] = m_cert.group(0).strip()
    m_region = region_re.search(html)
    if m_region:
        out["Region"] = m_region.group(2).strip()
    m_moq = moq_re.search(html)
    if m_moq:
        out["MOQ"] = m_moq.group(2).strip()
    m_price = price_re.search(html)
    if m_price:
        out["Price"] = m_price.group(1).strip()
    m_cname = cname_re.search(html)
    if m_cname:
        out["CNAME"] = m_cname.group(2).strip()

    soup = BeautifulSoup(html, "lxml")
    for a in soup.select("a[href]"):
        href = a.get("href", "")
        if "wa.me" in href or "whatsapp" in href:
            if not out["WhatsApp"]:
                out["WhatsApp"] = href
        if "t.me" in href or "telegram" in href:
            if not out["Telegram"]:
                out["Telegram"] = href
        if "weixin" in href or "wechat" in href:
            if not out["WeChat"]:
                out["WeChat"] = href
        if "mailto:" in href:
            em = href.split("mailto:")[1].split("?")[0]
            if not out["Email"]:
                out["Email"] = em

    return out


def format_for_silent_agent_cards(results: List[Dict], query: str) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML (string) ‚Äî –Ω–∞–±–æ—Ä –∫–∞—Ä—Ç–æ—á–µ–∫. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:
    - –±–µ–∑–æ–ø–∞—Å–Ω–æ –≥—Ä—É–∑–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç—ã
    - –≤—ã–≤–æ–¥–∏—Ç –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–µ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
    """
    if not results:
        return f"""
        <html><body style="font-family:-apple-system,Roboto,Arial;background:#f6f7fb;padding:18px">
        <div class="wrap" style="max-width:980px;margin:0 auto">
        <h1>‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –∑–∞–ø—Ä–æ—Å: ‚Äú{query}‚Äù</h1>
        </div></body></html>"""

    header = f"""
    <html><head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <style>
        body{{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial;background:#f6f7fb;color:#111;padding:18px}}
        .wrap{{max-width:980px;margin:0 auto}}
        .card{{background:#fff;border:1px solid #e0e4e8;border-radius:10px;padding:14px;margin-bottom:14px;box-shadow:0 1px 2px rgba(0,0,0,0.06)}}
        .meta{{color:#555;font-size:13px;margin-top:6px}}
        .link{{display:block;margin-top:8px}}
        .row{{margin-top:6px}}
        .label{{font-weight:600}}
      </style>
    </head><body><div class="wrap">
      <h1>üì° –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –∑–∞–ø—Ä–æ—Å: ‚Äú{query}‚Äù</h1>
      <div style="height:1px;background:#e5e7eb;margin:10px 0 14px;"></div>
    """

    cards = [header]

    for i, r in enumerate(results[:5], start=1):
        url = r.get("–°—Å—ã–ª–∫–∞") or ""
        page_html = safe_request(url) if url and url != "N/A" else ""
        c = extract_contacts(page_html, url)

        title = r.get("–ù–∞–∑–≤–∞–Ω–∏–µ") or f"Result {i}"
        source = r.get("–ò—Å—Ç–æ—á–Ω–∏–∫") or "‚Äî"
        region = c.get("Region") or r.get("–†–µ–≥–∏–æ–Ω") or "‚Äî"
        price  = c.get("Price")  or r.get("Price")  or "‚Äî"
        moq    = c.get("MOQ")    or r.get("MOQ")    or "‚Äî"
        certs  = c.get("Certificates") or r.get("Certificates") or "‚Äî"
        phone  = c.get("Phone") or r.get("Phone") or ""
        email  = c.get("Email") or r.get("Email") or ""
        wa     = c.get("WhatsApp") or r.get("WhatsApp") or ""
        tg     = c.get("Telegram") or r.get("Telegram") or ""
        wx     = c.get("WeChat") or r.get("WeChat") or ""

        contacts_parts = []
        if phone:   contacts_parts.append(f"–¢–µ–ª.: <a href='tel:{re.sub(r'\\D','',phone)}'>{phone}</a>")
        if email:   contacts_parts.append(f"Email: <a href='mailto:{email}'>{email}</a>")
        if wa:
            if wa.startswith("http"):
                contacts_parts.append(f"WhatsApp: <a href='{wa}' target='_blank'>{wa}</a>")
            else:
                contacts_parts.append(f"WhatsApp: <a href='https://wa.me/{re.sub(r'\\D','',wa)}' target='_blank'>{wa}</a>")
        if tg:
            if tg.startswith("http"):
                contacts_parts.append(f"Telegram: <a href='{tg}' target='_blank'>{tg}</a>")
            else:
                contacts_parts.append(f"Telegram: <a href='https://t.me/{tg}' target='_blank'>{tg}</a>")
        if wx:      contacts_parts.append(f"WeChat: {wx}")

        contacts_html = " | ".join(contacts_parts) if contacts_parts else "‚Äî"

        cards.append(f"""
        <div class="card">
          <div><span class="label">#{i}</span> <strong>{title}</strong></div>
          <div class="meta">–ò—Å—Ç–æ—á–Ω–∏–∫: {source}</div>
          <div class="row">–†–µ–≥–∏–æ–Ω: {region} &nbsp;|&nbsp; –¶–µ–Ω–∞: {price} &nbsp;|&nbsp; MOQ: {moq}</div>
          <div class="row">–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã: {certs}</div>
          <div class="row"><a class="link" href="{url}" target="_blank" rel="noopener noreferrer">üîó –°—Å—ã–ª–∫–∞ ‚Äî {url}</a></div>
          <div class="row"><b>–ö–æ–Ω—Ç–∞–∫—Ç—ã:</b> {contacts_html}</div>
        </div>
        """)

    cards.append("</div></body></html>")
    return "\n".join(cards)
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======