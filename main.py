from fastapi import FastAPI, Query
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup

# ====== Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ ======
import concurrent.futures
from urllib.parse import quote
import re, os
from typing import List, Dict

# ĞĞ±Ñ‰Ğ¸Ğµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/127.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru,en;q=0.9,zh;q=0.8",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}
SESSION = requests.Session()
SESSION.headers.update(DEFAULT_HEADERS)
TIMEOUT = 12
MAX_WORKERS = 16
MAX_RESULTS = 500

# ĞšĞ›Ğ®Ğ§Ğ˜ (ÑÑ‚Ğ°Ğ²ÑŒ Ğ½Ğ° Render â†’ Environment)
APIFY_TOKEN = os.getenv("APIFY_TOKEN", "").strip()
SERPAPI_KEY = os.getenv("SERPAPI_KEY", "").strip()
# ====== /Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ ======


app = FastAPI(
    title="DI-Agent SDK",
    description="Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² ĞšĞ¸Ñ‚Ğ°Ñ",
    version="2.0.0"
)

# ğŸ”¹ ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
class SearchRequest(BaseModel):
    query: str

# ğŸ”¹ Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
def safe_request(url):
    try:
        r = SESSION.get(url, timeout=TIMEOUT)
        if r.status_code == 200:
            return r.text
        else:
            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° {r.status_code} Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ {url}")
            return ""
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ {url}: {e}")
        return ""

# ğŸ”¹ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¹ (Ñ‚Ğ²Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ â€” ĞĞ• Ğ¢Ğ ĞĞ“ĞĞ›)
def parse_suppliers(html, selectors, source):
    if not html:
        return []

    soup = BeautifulSoup(html, "lxml")
    titles, links = [], []

    for sel in selectors:
        for t in soup.select(sel["title"]):
            title = t.get_text(strip=True)
            if title and title not in titles:
                titles.append(title)
        for a in soup.select(sel["link"]):
            href = a.get("href")
            if href and href not in links:
                links.append(href)

    suppliers = []
    for i in range(min(5, len(titles))):
        suppliers.append({
            "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ": titles[i],
            "Ğ¡ÑÑ‹Ğ»ĞºĞ°": links[i] if i < len(links) else "N/A",
            "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": source
        })
    return suppliers

# ====== Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ°Ğ¹Ñ‚Ñ‹ ======
GENERIC_SELECTORS = [
    {"title": "h2 a", "link": "h2 a"},
    {"title": ".title a", "link": ".title a"},
    {"title": ".product-title a", "link": ".product-title a"},
    {"title": ".company-name a", "link": ".company-name a"},
    {"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"},
    {"title": ".organic-gallery-title a", "link": ".organic-gallery-title a"},
    {"title": "h3 a", "link": "h3 a"},
    {"title": "a", "link": "a"},
]

def parse_flexible(html: str, source: str) -> List[Dict]:
    if not html:
        return []
    soup = BeautifulSoup(html, "lxml")
    results: List[Dict] = []
    seen = set()

    for sel in GENERIC_SELECTORS:
        for a in soup.select(sel["link"]):
            href = a.get("href", "").strip()
            title = a.get_text(" ", strip=True)
            if not href or not title:
                continue
            if href.startswith("//"):
                href = "https:" + href
            if not href.startswith("http"):
                continue
            key = (title, href)
            if key in seen:
                continue
            seen.add(key)
            results.append({
                "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ": title[:200],
                "Ğ¡ÑÑ‹Ğ»ĞºĞ°": href,
                "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": source
            })
            if len(results) >= 50:
                break
        if len(results) >= 50:
            break
    return results
# ====== /Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ ======


@app.get("/")
def root():
    return {
        "status": "âœ… DI-Agent SDK Ğ°ĞºÑ‚Ğ¸Ğ²ĞµĞ½",
        "docs": "/docs",
        "search_example": "/search?q=Ğ›Ğ¡Ğ¢Ğš"
    }


# ====== Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: ÑĞ¿Ğ¸ÑĞ¾Ğº 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² (B2B/ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ¸/Ğ¿Ğ¾Ğ¸ÑĞºĞ¸) ======
SOURCES: Dict[str, str] = {
    # ĞšÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ B2B
    "Alibaba": "https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&searchText={q}",
    "Made-in-China": "https://www.made-in-china.com/search?word={q}",
    "GlobalSources": "https://www.globalsources.com/searchList?query={q}",
    "1688 (via Baidu)": "https://www.baidu.com/s?wd={q}+site:1688.com",
    "HKTDC": "https://sourcing.hktdc.com/Search-Product?keyword={q}",
    "ECVV": "https://www.ecvv.com/catalog/{q}.html",
    "ECER": "https://www.ecer.com/search?kw={q}",
    "HC360": "https://s.hc360.com/seller/search.html?kwd={q}",
    "DHgate": "https://www.dhgate.com/wholesale/search.do?act=search&searchkey={q}",
    "YiwuGo": "https://en.yiwugo.com/search/{q}.html",
    "TradeKey": "https://www.tradekey.com/suppliers/{q}.html",
    "ExportHub": "https://www.exporthub.com/search?q={q}",
    "TradeWheel": "https://www.tradewheel.com/search/{q}/",
    "En.China.cn": "https://en.china.cn/search.html?searchKey={q}",
    "Hisupplier": "https://www.hisupplier.com/wholesale/{q}/",
    "Epoly": "https://www.etwinternational.com/search?kw={q}",
    "Globalspec": "https://www.globalspec.com/Search/Results?query={q}",
    "ThomasNet": "https://www.thomasnet.com/search.html?what={q}",
    "Kompass": "https://us.kompass.com/en/searchCompanies/companies/{q}/",
    "Qcc (companies)": "https://www.qcc.com/web/search?key={q}",
    "Tianyancha": "https://www.tianyancha.com/search?key={q}",

    # ĞœĞ°Ñ€ĞºĞµÑ‚Ñ‹ ĞšĞ¸Ñ‚Ğ°Ñ
    "JD": "https://search.jd.com/Search?keyword={q}",
    "Taobao": "https://s.taobao.com/search?q={q}",
    "Pinduoduo": "https://mobile.yangkeduo.com/search_result.html?search_key={q}",

    # ĞŸÑ€Ğ¾Ñ„. ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ¸/Ğ±Ğ¸Ñ€Ğ¶Ğ¸/Ñ‚ĞµĞ½Ğ´ĞµÑ€Ñ‹
    "MFG": "https://www.mfg.com/en/search/?q={q}",
    "AliExpress B2B": "https://www.aliexpress.com/wholesale?SearchText={q}",
    "Globalsources Verified": "https://www.globalsources.com/searchList?query={q}&verifiedSupplier=true",
    "Baidu Baike": "https://baike.baidu.com/search?word={q}",
    "Sohu": "https://www.sogou.com/web?query={q}",
    "Bing China": "https://cn.bing.com/search?q={q}",
    "Google (backup)": "https://www.google.com/search?q={q}",

    # Ğ•Ñ‰Ñ‘ B2B/Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€Ñ‹
    "E-WorldTrade": "https://www.eworldtrade.com/search/{q}/",
    "China.cn": "https://www.china.cn/search.html?searchKey={q}",
    "B2BManufactures": "https://www.manufacturers.com.tw/search.php?words={q}",
    "Maker-In-China": "https://www.maker-in-china.com/search.html?kw={q}",
    "Manufacturers Directory": "https://www.manufacturersdirectory.com/search?query={q}",
    "IndiaMART": "https://dir.indiamart.com/search.mp?ss={q}",
    "TradeIndia": "https://www.tradeindia.com/search.html?search_text={q}",
    "ECPlaza": "https://www.ecplaza.net/search/1?keyword={q}",
    "YellowPages": "https://www.yellowpages.com/search?search_terms={q}",
    "B2Brazil": "https://b2brazil.com/hotsite/search?term={q}",
    "B2BMit": "https://www.b2bmit.com/search.html?q={q}",
    "Globalsources Suppliers": "https://www.globalsources.com/suppliers?query={q}",
    "AliBaba Suppliers": "https://www.alibaba.com/company_directory/search/{q}.html",
    "CantonFair": "https://www.cantonfair.org.cn/en-US/search?key={q}",
    "HKTDC Suppliers": "https://sourcing.hktdc.com/en/supplier-search/{q}",
    "Europages": "https://www.europages.com/companies/{q}.html",
    "Kompass CN": "https://cn.kompass.com/en/searchCompanies/companies/{q}/",
    "Made-in-China Companies": "https://www.made-in-china.com/company-search/?word={q}",
    "MIC Verified": "https://www.made-in-china.com/company-search/?word={q}&select=verified",
    "GlobalMarket": "https://www.globalmarket.com/search/{q}.html",
    "EtradeAsia": "https://www.etradeasia.com/search?keyword={q}",
    "Mawoo": "https://www.made-in-asia.net/search?kw={q}",
    "EveryChina": "https://www.everychina.com/search.html?kw={q}",
    "ChinaProducts": "https://www.china-products-manufacturers.com/search?keyword={q}",
    "Crov": "https://www.crov.com/search?q={q}",
    "DiyTrade": "https://www.diytrade.com/china/search/products.do?keyword={q}",
    "Okchem": "https://www.okchem.com/search?keyword={q}",
    "ChemNet": "https://www.chemnet.com/global/en/search.html?keyword={q}",
    "Food2China": "https://www.food2china.com/search?keyword={q}",
    "PharmaSources": "https://www.pharmasources.com/searchResult?keyword={q}",
    "MedicaTradeFair": "https://www.medica-tradefair.com/vis/v1/en/search?term={q}",
    "HKTDC Products": "https://sourcing.hktdc.com/Search-Product?keyword={q}&productonly=1",
}

# Ğ¢Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¾Ğº (Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ â€” Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€)
SITE_SELECTORS: Dict[str, List[Dict[str, str]]] = {
    "Alibaba": [{"title": ".organic-gallery-title", "link": ".organic-gallery-title a"}],
    "Made-in-China": [{"title": ".company-name a", "link": ".company-name a"}],
    "GlobalSources": [{"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"}],
    "1688 (via Baidu)": [{"title": "h3.t a", "link": "h3.t a"}],
    "HKTDC": [{"title": ".product-name a, .cmpny-name a", "link": ".product-name a, .cmpny-name a"}],
    "ECER": [{"title": ".pro-title a, .supplier-name a", "link": ".pro-title a, .supplier-name a"}],
    "ECVV": [{"title": ".pro-title a, .company a", "link": ".pro-title a, .company a"}],
    "HC360": [{"title": ".search-list .title a", "link": ".search-list .title a"}],
    "DHgate": [{"title": ".item-title a", "link": ".item-title a"}],
    "YiwuGo": [{"title": ".title a", "link": ".title a"}],
    "TradeWheel": [{"title": ".item-title a", "link": ".item-title a"}],
    "ExportHub": [{"title": "h4.media-heading a", "link": "h4.media-heading a"}],
}

# Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ñ€ÑƒÑÑĞºĞ¸Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ°Ğ½Ğ³Ğ»-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹
def normalize_query(q: str) -> str:
    if re.search(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]", q):
        low = q.lower()
        mapping = {
            "Ñ„Ğ¸Ğ±Ñ€Ğ¾Ñ†ĞµĞ¼": "fiber cement panels",
            "Ğ¿ĞµĞ½Ğ¾Ğ±ĞµÑ‚Ğ¾Ğ½": "foam concrete",
            "Ğ»ÑÑ‚Ğº": "light gauge steel frame",
            "ÑÑĞ½Ğ´Ğ²Ğ¸Ñ‡": "sandwich panels",
            "Ğ¾Ñ†Ğ¸Ğ½ĞºĞ¾Ğ²Ğ°Ğ½": "galvanized steel",
            "Ğ°Ğ»ÑĞºĞ¾Ğ±Ğ¾Ğ½Ğ´": "aluminum composite panel",
        }
        for k, v in mapping.items():
            if k in low:
                return v
    return q

# ====== Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ğ¡Ğ¾Ñ†-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ/Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ğµ API ======
def serpapi_site_search(query: str, site: str, source_name: str) -> List[Dict]:
    """Site: search Ñ‡ĞµÑ€ĞµĞ· SerpAPI (Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ + ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ + Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸)."""
    if not SERPAPI_KEY:
        return []
    try:
        params = {
            "engine": "google",
            "q": f"site:{site} {query}",
            "num": 10,
            "api_key": SERPAPI_KEY
        }
        r = requests.get("https://serpapi.com/search.json", params=params, timeout=12)
        js = r.json()
        out = []
        for item in js.get("organic_results", []):
            title = item.get("title")
            link = item.get("link")
            if not title or not link:
                continue
            out.append({
                "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ": title,
                "Ğ¡ÑÑ‹Ğ»ĞºĞ°": link,
                "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": source_name
            })
        return out
    except Exception as e:
        print(f"âŒ SerpAPI {source_name}: {e}")
        return []

def apify_instagram_search(query: str) -> List[Dict]:
    """Public Instagram via Apify actor (Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹/Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²)."""
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~instagram-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "posts",
            "profilesType": "hashtag",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("caption") or it.get("ownerUsername") or "Instagram result"
            link = it.get("url") or it.get("shortCodeUrl")
            if not link:
                continue
            out.append({
                "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ": title[:200],
                "Ğ¡ÑÑ‹Ğ»ĞºĞ°": link,
                "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": "Instagram (Apify)"
            })
        return out
    except Exception as e:
        print(f"âŒ Apify Instagram: {e}")
        return []

def apify_tiktok_search(query: str) -> List[Dict]:
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~tiktok-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "videos",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("desc") or it.get("authorName") or "TikTok result"
            link = it.get("webVideoUrl") or it.get("shareUrl")
            if not link:
                continue
            out.append({
                "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ": title[:200],
                "Ğ¡ÑÑ‹Ğ»ĞºĞ°": link,
                "Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": "TikTok (Apify)"
            })
        return out
    except Exception as e:
        print(f"âŒ Apify TikTok: {e}")
        return []

def social_collect(query: str) -> List[Dict]:
    q = normalize_query(query)
    results = []

    # Instagram/TikTok (Ñ‡ĞµÑ€ĞµĞ· Apify)
    results += apify_instagram_search(q)
    results += apify_tiktok_search(q)

    # --- Ğ§ĞµÑ€ĞµĞ· SerpAPI site: ---
    # NEW â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
    social_sites = {
        # ğŸ”¹ Ğ—Ğ°Ğ¿Ğ°Ğ´Ğ½Ñ‹Ğµ ÑĞ¾Ñ†ÑĞµÑ‚Ğ¸
        "Telegram": "t.me",
        "WhatsApp": "wa.me",
        "YouTube": "youtube.com",
        "Facebook": "facebook.com",
        "Twitter (X)": "x.com",
        "Pinterest": "pinterest.com",
        "Reddit": "reddit.com",
        "LinkedIn": "linkedin.com/company",
        "Threads (Meta)": "threads.net",
        "Instagram (backup)": "instagram.com",
        "Snapchat": "snapchat.com",
        "Twitch": "twitch.tv",
        "Discord": "discord.com",
        "Tumblr": "tumblr.com",
        "Medium": "medium.com",

        # ğŸ”¹ Ğ’Ğ¾ÑÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ
        "WeChat": "weixin.qq.com",
        "QQ": "qq.com",
        "Weibo": "weibo.com",
        "Douyin (CN TikTok)": "douyin.com",
        "Bilibili": "bilibili.com",
        "Zhihu": "zhihu.com",
        "Youku": "youku.com",
        "Xiaohongshu (RED)": "xiaohongshu.com",
        "Taobao Live": "live.taobao.com",
        "Kuaishou": "kuaishou.com",

        # ğŸ”¹ Ğ Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğµ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ
        "VK": "vk.com",
        "Odnoklassniki": "ok.ru",
        "Rutube": "rutube.ru",
        "Yappy": "yappy.media",
        "Dzen": "dzen.ru",
    }

    for name, site in social_sites.items():
        results += serpapi_site_search(q, site, name)
    # NEW â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘

    return results
# ====== /Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ ======

def _fetch_one(name: str, url: str) -> List[Dict]:
    html = safe_request(url)
    if name in SITE_SELECTORS:
        return parse_suppliers(html, SITE_SELECTORS[name], name)
    return parse_flexible(html, name)

def extended_collect(query: str) -> List[Dict]:
    q_norm = normalize_query(query)
    q_enc = quote(q_norm)

    tasks = {}
    results: List[Dict] = []

    # 1) Ğ¡Ğ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (API) â€” Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸
    try:
        social = social_collect(q_norm)
        if social:
            print(f"âœ… SOCIAL: {len(social)}")
            results.extend(social)
    except Exception as e:
        print(f"âŒ SOCIAL error: {e}")

    # 2) B2B/ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ¸/Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for name, tmpl in SOURCES.items():
            url = tmpl.format(q=q_enc)
            tasks[ex.submit(_fetch_one, name, url)] = name

        for fut in concurrent.futures.as_completed(tasks):
            name = tasks[fut]
            try:
                chunk = fut.result() or []
                if chunk:
                    print(f"âœ… {name}: {len(chunk)}")
                results.extend(chunk)
            except Exception as e:
                print(f"âŒ {name}: {e}")

    # Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑÑÑ‹Ğ»ĞºĞµ
    uniq, seen = [], set()
    for item in results:
        link = (item.get("Ğ¡ÑÑ‹Ğ»ĞºĞ°") or "").strip()
        if not link or link in seen:
            continue
        seen.add(link)
        uniq.append(item)
        if len(uniq) >= MAX_RESULTS:
            break

    return uniq


# ğŸ”¹ ĞĞ‘ĞĞĞ’Ğ›ĞĞĞĞ«Ğ™ /search: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ (70+),
#    ĞµÑĞ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾ â€” Ñ‚Ğ²Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº (4 Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ĞºĞ¸)
@app.get("/search_all")
def search_all(q: str = Query(..., description="ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼")):
    data = extended_collect(q)
    text_output = format_for_silent_agent_cards(data, q)
    return text_output
        }

    results = []
    html = safe_request(f"https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&searchText={quote(normalize_query(q))}")
    results += parse_suppliers(html, [{"title": "h2.title, .organic-gallery-title", "link": "h2.title a, .organic-gallery-title a"}], "Alibaba")

    html = safe_request(f"https://www.made-in-china.com/search?word={quote(normalize_query(q))}")
    results += parse_suppliers(html, [{"title": ".company-name a", "link": ".company-name a"}], "Made-in-China")

    html = safe_request(f"https://www.globalsources.com/searchList?query={quote(normalize_query(q))}")
    results += parse_suppliers(html, [{"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"}], "GlobalSources")

    html = safe_request(f"https://www.baidu.com/s?wd={quote(normalize_query(q))}+site:1688.com")
    results += parse_suppliers(html, [{"title": "h3.t a", "link": "h3.t a"}], "1688")

    if not results:
        return {"status": "error", "query": q, "results": []}

    return {
        "status": "ok",
        "query": q,
        "count": len(results),
        "results": results[:50]
    }

# ğŸ”¹ ĞŸĞ Ğ¯ĞœĞĞ™ ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° (Ğ´Ğ»Ñ GPT)
@app.get("/search_all")
def search_all(q: str = Query(..., description="ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼")):
    data = extended_collect(q)
    return {
        "status": "ok" if data else "error",
        "query": q,
        "count": len(data),
        "results": data[:MAX_RESULTS]
    }
# ====== KEEP-ALIVE (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Render Ğ½Ğµ Ğ·Ğ°ÑÑ‹Ğ¿Ğ°Ğ») ======
import threading, time

def keep_alive():
    """ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¸Ğ½Ğ³ÑƒĞµÑ‚ ÑĞ°Ğ¼ ÑĞµÑ€Ğ²ĞµÑ€, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Render Ğ½Ğµ Ğ·Ğ°ÑÑ‹Ğ¿Ğ°Ğ»."""
    while True:
        try:
            requests.get("https://di-agent-sdk.onrender.com/", timeout=5)
            print("ğŸ”„ Keep-alive ping OK")
        except Exception as e:
            print(f"âš ï¸ Keep-alive error: {e}")
        time.sleep(300)  # ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚

# Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° FastAPI
threading.Thread(target=keep_alive, daemon=True).start()
# ====== /KEEP-ALIVE ======# ====== Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ: Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº SILENT SUPPLIER AGENT ======
import random
from datetime import datetime

def format_for_silent_agent_cards(results: List[Dict], query: str) -> str:
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ‚ĞµĞºÑÑ‚ Ñ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¼ 'ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼' Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ´Ğ°"""
    if not results:
        return f"âŒ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ğŸ“¡ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ â€” Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ: \"{query}\""

    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"]
    out = [f"ğŸ“¡ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¿Ğ¾ 70+ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ â€” Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ: \"{query}\"\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"]

    for i, r in enumerate(results[:5]):
        out.append(f"""{medals[i]} **TOP {i+1} â€” {r.get('ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Unknown')}**
ğŸŒ **Ğ ĞµĞ³Ğ¸Ğ¾Ğ½:** {r.get('Ğ ĞµĞ³Ğ¸Ğ¾Ğ½', 'â€”')}
ğŸ·ï¸ **ĞŸÑ€Ğ¾Ğ´ÑƒĞºÑ‚:** {r.get('Product', query)}
ğŸ’° **Ğ¦ĞµĞ½Ğ°:** {r.get('Price', 'â€”')}
ğŸ“¦ **MOQ:** {r.get('MOQ', 'â€”')}
ğŸ§¾ **Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹:** {r.get('Certificates', 'â€”')}
ğŸ“ **ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹:** WeChat: {r.get('WeChat', '')} | WhatsApp: {r.get('WhatsApp', '')} | Telegram: {r.get('Telegram', '')} | Email: {r.get('Email', '')} | Phone: {r.get('Phone', '')} | Website: {r.get('Ğ¡ÑÑ‹Ğ»ĞºĞ°', '')}
ğŸ§  **Ğ ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³:** {r.get('Rating', 'â€”')} / 100
ğŸ”— **Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:** {r.get('Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº', 'â€”')}
ğŸ–¼ï¸ [Image]({r.get('Image', 'https://via.placeholder.com/400x300?text=Supplier')})
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€""")

    return "\n\n".join(out)
# ====== /Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•ĞĞ ======