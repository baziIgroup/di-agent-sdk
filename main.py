from fastapi import FastAPI, Query
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup

# ====== –î–û–ë–ê–í–õ–ï–ù–û ======
import concurrent.futures
from urllib.parse import quote
import re, os
from typing import List, Dict

# –û–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã –º–µ–Ω—å—à–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/127.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ru,en;q=0.9,zh;q=0.8",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}
SESSION = requests.Session()
SESSION.headers.update(DEFAULT_HEADERS)
TIMEOUT = 12
MAX_WORKERS = 16
MAX_RESULTS = 500

# –ö–õ–Æ–ß–ò (—Å—Ç–∞–≤—å –Ω–∞ Render ‚Üí Environment)
APIFY_TOKEN = os.getenv("APIFY_TOKEN", "").strip()
SERPAPI_KEY = os.getenv("SERPAPI_KEY", "").strip()
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======


app = FastAPI(
    title="DI-Agent SDK",
    description="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –ö–∏—Ç–∞—è",
    version="2.0.0"
)

# üîπ –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
class SearchRequest(BaseModel):
    query: str

# üîπ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
def safe_request(url):
    try:
        r = SESSION.get(url, timeout=TIMEOUT)
        if r.status_code == 200:
            return r.text
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {r.status_code} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
            return ""
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
        return ""

# üîπ –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π (—Ç–≤–æ–π –∏—Å—Ö–æ–¥–Ω—ã–π –ø–∞—Ä—Å–µ—Ä ‚Äî –ù–ï –¢–†–û–ì–ê–õ)
def parse_suppliers(html, selectors, source):
    if not html:
        return []

    soup = BeautifulSoup(html, "lxml")
    titles, links = [], []

    for sel in selectors:
        for t in soup.select(sel["title"]):
            title = t.get_text(strip=True)
            if title and title not in titles:
                titles.append(title)
        for a in soup.select(sel["link"]):
            href = a.get("href")
            if href and href not in links:
                links.append(href)

    suppliers = []
    for i in range(min(5, len(titles))):
        suppliers.append({
            "–ù–∞–∑–≤–∞–Ω–∏–µ": titles[i],
            "–°—Å—ã–ª–∫–∞": links[i] if i < len(links) else "N/A",
            "–ò—Å—Ç–æ—á–Ω–∏–∫": source
        })
    return suppliers

# ====== –î–û–ë–ê–í–õ–ï–ù–û: –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —Å–∞–π—Ç—ã ======
GENERIC_SELECTORS = [
    {"title": "h2 a", "link": "h2 a"},
    {"title": ".title a", "link": ".title a"},
    {"title": ".product-title a", "link": ".product-title a"},
    {"title": ".company-name a", "link": ".company-name a"},
    {"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"},
    {"title": ".organic-gallery-title a", "link": ".organic-gallery-title a"},
    {"title": "h3 a", "link": "h3 a"},
    {"title": "a", "link": "a"},
]

def parse_flexible(html: str, source: str) -> List[Dict]:
    if not html:
        return []
    soup = BeautifulSoup(html, "lxml")
    results: List[Dict] = []
    seen = set()

    for sel in GENERIC_SELECTORS:
        for a in soup.select(sel["link"]):
            href = a.get("href", "").strip()
            title = a.get_text(" ", strip=True)
            if not href or not title:
                continue
            if href.startswith("//"):
                href = "https:" + href
            if not href.startswith("http"):
                continue
            key = (title, href)
            if key in seen:
                continue
            seen.add(key)
            results.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": href,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": source
            })
            if len(results) >= 50:
                break
        if len(results) >= 50:
            break
    return results
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======


@app.get("/")
def root():
    return {
        "status": "‚úÖ DI-Agent SDK –∞–∫—Ç–∏–≤–µ–Ω",
        "docs": "/docs",
        "search_example": "/search?q=–õ–°–¢–ö"
    }


# ====== –î–û–ë–ê–í–õ–ï–ù–û: —Å–ø–∏—Å–æ–∫ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (B2B/–∫–∞—Ç–∞–ª–æ–≥–∏/–ø–æ–∏—Å–∫–∏) ======
SOURCES: Dict[str, str] = {
    # –ö—Ä—É–ø–Ω—ã–µ B2B
    "Alibaba": "https://www.alibaba.com/trade/search?fsb=y&IndexArea=product_en&searchText={q}",
    "Made-in-China": "https://www.made-in-china.com/search?word={q}",
    "GlobalSources": "https://www.globalsources.com/searchList?query={q}",
    "1688 (via Baidu)": "https://www.baidu.com/s?wd={q}+site:1688.com",
    "HKTDC": "https://sourcing.hktdc.com/Search-Product?keyword={q}",
    "ECVV": "https://www.ecvv.com/catalog/{q}.html",
    "ECER": "https://www.ecer.com/search?kw={q}",
    "HC360": "https://s.hc360.com/seller/search.html?kwd={q}",
    "DHgate": "https://www.dhgate.com/wholesale/search.do?act=search&searchkey={q}",
    "YiwuGo": "https://en.yiwugo.com/search/{q}.html",
    "TradeKey": "https://www.tradekey.com/suppliers/{q}.html",
    "ExportHub": "https://www.exporthub.com/search?q={q}",
    "TradeWheel": "https://www.tradewheel.com/search/{q}/",
    "En.China.cn": "https://en.china.cn/search.html?searchKey={q}",
    "Hisupplier": "https://www.hisupplier.com/wholesale/{q}/",
    "Epoly": "https://www.etwinternational.com/search?kw={q}",
    "Globalspec": "https://www.globalspec.com/Search/Results?query={q}",
    "ThomasNet": "https://www.thomasnet.com/search.html?what={q}",
    "Kompass": "https://us.kompass.com/en/searchCompanies/companies/{q}/",
    "Qcc (companies)": "https://www.qcc.com/web/search?key={q}",
    "Tianyancha": "https://www.tianyancha.com/search?key={q}",

    # –ú–∞—Ä–∫–µ—Ç—ã –ö–∏—Ç–∞—è
    "JD": "https://search.jd.com/Search?keyword={q}",
    "Taobao": "https://s.taobao.com/search?q={q}",
    "Pinduoduo": "https://mobile.yangkeduo.com/search_result.html?search_key={q}",

    # –ü—Ä–æ—Ñ. –∫–∞—Ç–∞–ª–æ–≥–∏/–±–∏—Ä–∂–∏/—Ç–µ–Ω–¥–µ—Ä—ã
    "MFG": "https://www.mfg.com/en/search/?q={q}",
    "AliExpress B2B": "https://www.aliexpress.com/wholesale?SearchText={q}",
    "Globalsources Verified": "https://www.globalsources.com/searchList?query={q}&verifiedSupplier=true",
    "Baidu Baike": "https://baike.baidu.com/search?word={q}",
    "Sohu": "https://www.sogou.com/web?query={q}",
    "Bing China": "https://cn.bing.com/search?q={q}",
    "Google (backup)": "https://www.google.com/search?q={q}",

    # –ï—â—ë B2B/–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
    "E-WorldTrade": "https://www.eworldtrade.com/search/{q}/",
    "China.cn": "https://www.china.cn/search.html?searchKey={q}",
    "B2BManufactures": "https://www.manufacturers.com.tw/search.php?words={q}",
    "Maker-In-China": "https://www.maker-in-china.com/search.html?kw={q}",
    "Manufacturers Directory": "https://www.manufacturersdirectory.com/search?query={q}",
    "IndiaMART": "https://dir.indiamart.com/search.mp?ss={q}",
    "TradeIndia": "https://www.tradeindia.com/search.html?search_text={q}",
    "ECPlaza": "https://www.ecplaza.net/search/1?keyword={q}",
    "YellowPages": "https://www.yellowpages.com/search?search_terms={q}",
    "B2Brazil": "https://b2brazil.com/hotsite/search?term={q}",
    "B2BMit": "https://www.b2bmit.com/search.html?q={q}",
    "Globalsources Suppliers": "https://www.globalsources.com/suppliers?query={q}",
    "AliBaba Suppliers": "https://www.alibaba.com/company_directory/search/{q}.html",
    "CantonFair": "https://www.cantonfair.org.cn/en-US/search?key={q}",
    "HKTDC Suppliers": "https://sourcing.hktdc.com/en/supplier-search/{q}",
    "Europages": "https://www.europages.com/companies/{q}.html",
    "Kompass CN": "https://cn.kompass.com/en/searchCompanies/companies/{q}/",
    "Made-in-China Companies": "https://www.made-in-china.com/company-search/?word={q}",
    "MIC Verified": "https://www.made-in-china.com/company-search/?word={q}&select=verified",
    "GlobalMarket": "https://www.globalmarket.com/search/{q}.html",
    "EtradeAsia": "https://www.etradeasia.com/search?keyword={q}",
    "Mawoo": "https://www.made-in-asia.net/search?kw={q}",
    "EveryChina": "https://www.everychina.com/search.html?kw={q}",
    "ChinaProducts": "https://www.china-products-manufacturers.com/search?keyword={q}",
    "Crov": "https://www.crov.com/search?q={q}",
    "DiyTrade": "https://www.diytrade.com/china/search/products.do?keyword={q}",
    "Okchem": "https://www.okchem.com/search?keyword={q}",
    "ChemNet": "https://www.chemnet.com/global/en/search.html?keyword={q}",
    "Food2China": "https://www.food2china.com/search?keyword={q}",
    "PharmaSources": "https://www.pharmasources.com/searchResult?keyword={q}",
    "MedicaTradeFair": "https://www.medica-tradefair.com/vis/v1/en/search?term={q}",
    "HKTDC Products": "https://sourcing.hktdc.com/Search-Product?keyword={q}&productonly=1",
}

# –¢–æ—á–µ—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫ (–æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –≥–∏–±–∫–∏–π –ø–∞—Ä—Å–µ—Ä)
SITE_SELECTORS: Dict[str, List[Dict[str, str]]] = {
    "Alibaba": [{"title": ".organic-gallery-title", "link": ".organic-gallery-title a"}],
    "Made-in-China": [{"title": ".company-name a", "link": ".company-name a"}],
    "GlobalSources": [{"title": "a.gs-product-card__name", "link": "a.gs-product-card__name"}],
    "1688 (via Baidu)": [{"title": "h3.t a", "link": "h3.t a"}],
    "HKTDC": [{"title": ".product-name a, .cmpny-name a", "link": ".product-name a, .cmpny-name a"}],
    "ECER": [{"title": ".pro-title a, .supplier-name a", "link": ".pro-title a, .supplier-name a"}],
    "ECVV": [{"title": ".pro-title a, .company a", "link": ".pro-title a, .company a"}],
    "HC360": [{"title": ".search-list .title a", "link": ".search-list .title a"}],
    "DHgate": [{"title": ".item-title a", "link": ".item-title a"}],
    "YiwuGo": [{"title": ".title a", "link": ".title a"}],
    "TradeWheel": [{"title": ".item-title a", "link": ".item-title a"}],
    "ExportHub": [{"title": "h4.media-heading a", "link": "h4.media-heading a"}],
}

# –ë—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–µ–≤–æ–¥ —á–∞—Å—Ç—ã—Ö —Ä—É—Å—Å–∫–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ –∞–Ω–≥–ª-–∑–∞–ø—Ä–æ—Å—ã
def normalize_query(q: str) -> str:
    if re.search(r"[–ê-–Ø–∞-—è–Å—ë]", q):
        low = q.lower()
        mapping = {
            "—Ñ–∏–±—Ä–æ—Ü–µ–º": "fiber cement panels",
            "–ø–µ–Ω–æ–±–µ—Ç–æ–Ω": "foam concrete",
            "–ª—Å—Ç–∫": "light gauge steel frame",
            "—Å—ç–Ω–¥–≤–∏—á": "sandwich panels",
            "–æ—Ü–∏–Ω–∫–æ–≤–∞–Ω": "galvanized steel",
            "–∞–ª—é–∫–æ–±–æ–Ω–¥": "aluminum composite panel",
        }
        for k, v in mapping.items():
            if k in low:
                return v
    return q

# ====== –î–û–ë–ê–í–õ–ï–ù–û: –°–æ—Ü-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ/—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ API ======
def serpapi_site_search(query: str, site: str, source_name: str) -> List[Dict]:
    """Site: search —á–µ—Ä–µ–∑ SerpAPI (—Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ + —Å–Ω–∏–ø–ø–µ—Ç—ã + –∏–Ω–æ–≥–¥–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏)."""
    if not SERPAPI_KEY:
        return []
    try:
        params = {
            "engine": "google",
            "q": f"site:{site} {query}",
            "num": 10,
            "api_key": SERPAPI_KEY
        }
        r = requests.get("https://serpapi.com/search.json", params=params, timeout=12)
        js = r.json()
        out = []
        for item in js.get("organic_results", []):
            title = item.get("title")
            link = item.get("link")
            if not title or not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title,
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": source_name
            })
        return out
    except Exception as e:
        print(f"‚ùå SerpAPI {source_name}: {e}")
        return []

def apify_instagram_search(query: str) -> List[Dict]:
    """Public Instagram via Apify actor (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É–±–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π/–ø–æ—Å—Ç–æ–≤)."""
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~instagram-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "posts",
            "profilesType": "hashtag",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("caption") or it.get("ownerUsername") or "Instagram result"
            link = it.get("url") or it.get("shortCodeUrl")
            if not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": "Instagram (Apify)"
            })
        return out
    except Exception as e:
        print(f"‚ùå Apify Instagram: {e}")
        return []

def apify_tiktok_search(query: str) -> List[Dict]:
    if not APIFY_TOKEN:
        return []
    try:
        url = f"https://api.apify.com/v2/acts/apify~tiktok-scraper/run-sync-get-dataset-items?token={APIFY_TOKEN}"
        payload = {
            "search": query,
            "resultsType": "videos",
            "resultsLimit": 10
        }
        r = requests.post(url, json=payload, timeout=30)
        items = r.json() if r.status_code == 200 else []
        out = []
        for it in items:
            title = it.get("desc") or it.get("authorName") or "TikTok result"
            link = it.get("webVideoUrl") or it.get("shareUrl")
            if not link:
                continue
            out.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ": title[:200],
                "–°—Å—ã–ª–∫–∞": link,
                "–ò—Å—Ç–æ—á–Ω–∏–∫": "TikTok (Apify)"
            })
        return out
    except Exception as e:
        print(f"‚ùå Apify TikTok: {e}")
        return []

def social_collect(query: str) -> List[Dict]:
    q = normalize_query(query)
    results = []

    # Instagram/TikTok (—á–µ—Ä–µ–∑ Apify)
    results += apify_instagram_search(q)
    results += apify_tiktok_search(q)

    # --- –ß–µ—Ä–µ–∑ SerpAPI site: ---
    # NEW ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì
    social_sites = {
        # üîπ –ó–∞–ø–∞–¥–Ω—ã–µ —Å–æ—Ü—Å–µ—Ç–∏
        "Telegram": "t.me",
        "WhatsApp": "wa.me",
        "YouTube": "youtube.com",
        "Facebook": "facebook.com",
        "Twitter (X)": "x.com",
        "Pinterest": "pinterest.com",
        "Reddit": "reddit.com",
        "LinkedIn": "linkedin.com/company",
        "Threads (Meta)": "threads.net",
        "Instagram (backup)": "instagram.com",
        "Snapchat": "snapchat.com",
        "Twitch": "twitch.tv",
        "Discord": "discord.com",
        "Tumblr": "tumblr.com",
        "Medium": "medium.com",

        # üîπ –í–æ—Å—Ç–æ—á–Ω—ã–µ –∏ –∫–∏—Ç–∞–π—Å–∫–∏–µ
        "WeChat": "weixin.qq.com",
        "QQ": "qq.com",
        "Weibo": "weibo.com",
        "Douyin (CN TikTok)": "douyin.com",
        "Bilibili": "bilibili.com",
        "Zhihu": "zhihu.com",
        "Youku": "youku.com",
        "Xiaohongshu (RED)": "xiaohongshu.com",
        "Taobao Live": "live.taobao.com",
        "Kuaishou": "kuaishou.com",

        # üîπ –†–æ—Å—Å–∏–π—Å–∫–∏–µ –∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ
        "VK": "vk.com",
        "Odnoklass–Ω–∏–∫–∏": "ok.ru",
        "Rutube": "rutube.ru",
        "Yappy": "yappy.media",
        "Dzen": "dzen.ru",
    }

    for name, site in social_sites.items():
        results += serpapi_site_search(q, site, name)
    # NEW ‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë

    return results
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======

def _fetch_one(name: str, url: str) -> List[Dict]:
    html = safe_request(url)
    if name in SITE_SELECTORS:
        return parse_suppliers(html, SITE_SELECTORS[name], name)
    return parse_flexible(html, name)

def extended_collect(query: str) -> List[Dict]:
    q_norm = normalize_query(query)
    q_enc = quote(q_norm)

    tasks = {}
    results: List[Dict] = []

    # 1) –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (API) ‚Äî —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    try:
        social = social_collect(q_norm)
        if social:
            print(f"‚úÖ SOCIAL: {len(social)}")
            results.extend(social)
    except Exception as e:
        print(f"‚ùå SOCIAL error: {e}")

    # 2) B2B/–∫–∞—Ç–∞–ª–æ–≥–∏/–ø–æ–∏—Å–∫–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        for name, tmpl in SOURCES.items():
            url = tmpl.format(q=q_enc)
            tasks[ex.submit(_fetch_one, name, url)] = name

        for fut in concurrent.futures.as_completed(tasks):
            name = tasks[fut]
            try:
                chunk = fut.result() or []
                if chunk:
                    print(f"‚úÖ {name}: {len(chunk)}")
                results.extend(chunk)
            except Exception as e:
                print(f"‚ùå {name}: {e}")

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–æ–ª–Ω–æ–π —Å—Å—ã–ª–∫–µ
    uniq, seen = [], set()
    for item in results:
        link = (item.get("–°—Å—ã–ª–∫–∞") or "").strip()
        if not link or link in seen:
            continue
        seen.add(link)
        uniq.append(item)
        if len(uniq) >= MAX_RESULTS:
            break
    # <<< –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –≤–æ–∑–≤—Ä–∞—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–Ω–µ –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞)
    return uniq


# üîπ –ü–†–Ø–ú–û–ô —ç–Ω–¥–ø–æ–∏–Ω—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–±–æ—Ä–∞ (–¥–ª—è GPT)
from fastapi.responses import PlainTextResponse

@app.get("/search_all", response_class=PlainTextResponse)
def search_all(q: str = Query(..., description="–ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º")):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç ‚Äî –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–æ—Å q (–Ω–∞–ø—Ä–∏–º–µ—Ä, LGSF)"""
    data = extended_collect(q)
    text_output = format_for_silent_agent_cards(data, q)
    return text_output


# ====== KEEP-ALIVE (—á—Ç–æ–±—ã Render –Ω–µ –∑–∞—Å—ã–ø–∞–ª) ======
import threading, time

def keep_alive():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–∏–Ω–≥—É–µ—Ç —Å–∞–º —Å–µ—Ä–≤–µ—Ä, —á—Ç–æ–±—ã Render –Ω–µ –∑–∞—Å—ã–ø–∞–ª."""
    while True:
        try:
            requests.get("https://di-agent-sdk.onrender.com/", timeout=5)
            print("üîÑ Keep-alive ping OK")
        except Exception as e:
            print(f"‚ö†Ô∏è Keep-alive error: {e}")
        time.sleep(300)  # –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞ FastAPI
threading.Thread(target=keep_alive, daemon=True).start()
# ====== /KEEP-ALIVE ======# ====== –î–û–ë–ê–í–õ–ï–ù–û: —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ –∫–∞—Ä—Ç–æ—á–µ–∫ SILENT SUPPLIER AGENT ======
import random
from datetime import datetime

def format_for_silent_agent_cards(results: List[Dict], query: str) -> str:
    """–°–æ–∑–¥–∞—ë—Ç —Ç–µ–∫—Å—Ç —Å —ç–º–æ–¥–∑–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º '–ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º' –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞"""
    if not results:
        return f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö. üì° –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –∑–∞–ø—Ä–æ—Å: \"{query}\""

    medals = ["ü•á", "ü•à", "ü•â", "4Ô∏è‚É£", "5Ô∏è‚É£"]
    out = [f"üì° –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –∑–∞–ø—Ä–æ—Å: \"{query}\"\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"]

    for i, r in enumerate(results[:5]):
        out.append(f"""{medals[i]} **TOP {i+1} ‚Äî {r.get('–ù–∞–∑–≤–∞–Ω–∏–µ', 'Unknown')}**
üåç **–†–µ–≥–∏–æ–Ω:** {r.get('–†–µ–≥–∏–æ–Ω', '‚Äî')}
üè∑Ô∏è **–ü—Ä–æ–¥—É–∫—Ç:** {r.get('Product', query)}
üí∞ **–¶–µ–Ω–∞:** {r.get('Price', '‚Äî')}
üì¶ **MOQ:** {r.get('MOQ', '‚Äî')}
üßæ **–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã:** {r.get('Certificates', '‚Äî')}
üìû **–ö–æ–Ω—Ç–∞–∫—Ç—ã:** WeChat: {r.get('WeChat', '')} | WhatsApp: {r.get('WhatsApp', '')} | Telegram: {r.get('Telegram', '')} | Email: {r.get('Email', '')} | Phone: {r.get('Phone', '')} | Website: {r.get('–°—Å—ã–ª–∫–∞', '')}
üß† **–†–µ–π—Ç–∏–Ω–≥:** {r.get('Rating', '‚Äî')} / 100
üîó **–ò—Å—Ç–æ—á–Ω–∏–∫:** {r.get('–ò—Å—Ç–æ—á–Ω–∏–∫', '‚Äî')}
üñºÔ∏è [Image]({r.get('Image', 'https://via.placeholder.com/400x300?text=Supplier')})
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ""")

    return "\n\n".join(out)
# ====== /–î–û–ë–ê–í–õ–ï–ù–û ======
# ================== –ö–û–ù–ï–¶ –¢–í–û–ï–ì–û –ö–û–î–ê –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô ==================


# ================== –î–û–ë–ê–í–õ–ï–ù–û –ù–ò–ñ–ï: –ö–ê–†–¢–û–ß–ö–ò –í HTML ==================
# (–Ω–∏—á–µ–≥–æ –≤—ã—à–µ –Ω–µ –º–µ–Ω—è–µ—Ç; –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —Å HTML-–≤—ë—Ä—Å—Ç–∫–æ–π –∫–∞—Ä—Ç–æ—á–µ–∫)
from fastapi.responses import HTMLResponse as _HTMLResponse

@app.get("/search_all_html", response_class=_HTMLResponse)
def search_all_html(q: str = Query(..., description="–ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (HTML –∫–∞—Ä—Ç–æ—á–∫–∏)")):
    data = extended_collect(q)
    if not data:
        return f"""
        <html><body style="font-family:Arial;max-width:900px;margin:40px auto;">
            <h2>‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å—É: <b>{q}</b></h2>
        </body></html>
        """

    cards_html = []
    for i, r in enumerate(data[:5], start=1):
        name = r.get("–ù–∞–∑–≤–∞–Ω–∏–µ", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
        link = r.get("–°—Å—ã–ª–∫–∞", "")
        src  = r.get("–ò—Å—Ç–æ—á–Ω–∏–∫", "‚Äî")

        # –î–æ–ø. –ø–æ–ª—è ‚Äî –µ—Å–ª–∏ –∫–æ–≥–¥–∞-—Ç–æ –ø–æ—è–≤—è—Ç—Å—è –≤ –¥–∞–Ω–Ω—ã—Ö, –±—É–¥—É—Ç –ø–æ–∫–∞–∑–∞–Ω—ã; –∏–Ω–∞—á–µ "‚Äî"
        region = r.get("–†–µ–≥–∏–æ–Ω", "‚Äî")
        price  = r.get("Price", "‚Äî")
        moq    = r.get("MOQ", "‚Äî")
        certs  = r.get("Certificates", "‚Äî")
        phone  = r.get("Phone", "‚Äî")
        email  = r.get("Email", "‚Äî")
        wa     = r.get("WhatsApp", "‚Äî")
        tg     = r.get("Telegram", "‚Äî")

        cards_html.append(f"""
        <div style="border:1px solid #e5e7eb;border-radius:12px;padding:16px;margin:18px 0;
                    box-shadow:0 2px 6px rgba(0,0,0,0.06);font-family:Arial,Helvetica,sans-serif;">
            <div style="display:flex;align-items:center;gap:8px;">
                <div style="font-size:20px;font-weight:700;">#{i}</div>
                <div style="font-size:18px;font-weight:700;line-height:1.2;">{name}</div>
            </div>
            <div style="margin-top:8px;color:#374151;">
                <div><b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> {src}</div>
                <div><b>–°—Å—ã–ª–∫–∞:</b> <a href="{link}" target="_blank" rel="noopener noreferrer">{link}</a></div>
                <div style="margin-top:6px;"><b>–†–µ–≥–∏–æ–Ω:</b> {region} &nbsp;|&nbsp; <b>–¶–µ–Ω–∞:</b> {price} &nbsp;|&nbsp; <b>MOQ:</b> {moq}</div>
                <div><b>–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã:</b> {certs}</div>
                <div style="margin-top:6px;"><b>–ö–æ–Ω—Ç–∞–∫—Ç—ã:</b> –¢–µ–ª.: {phone} &nbsp;|&nbsp; Email: {email} &nbsp;|&nbsp; WhatsApp: {wa} &nbsp;|&nbsp; Telegram: {tg}</div>
            </div>
        </div>
        """)

    html = f"""
    <html>
      <head>
        <meta charset="utf-8" />
        <title>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã ‚Äî {q}</title>
      </head>
      <body style="max-width:900px;margin:32px auto;padding:0 12px;font-family:Arial,Helvetica,sans-serif;color:#111827;">
        <h2 style="margin-bottom:8px;">üì° –ü–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ 70+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –∑–∞–ø—Ä–æ—Å: ‚Äú{q}‚Äù</h2>
        <div style="height:1px;background:#e5e7eb;margin:12px 0 20px;"></div>
        {''.join(cards_html)}
      </body>
    </html>
    """
    return html
# ================== /–ö–ê–†–¢–û–ß–ö–ò –í HTML ==================